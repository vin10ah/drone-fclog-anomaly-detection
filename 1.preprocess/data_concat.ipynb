{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73668142",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5352370c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ab_dir_lst = glob.glob(\"./data/abnormal/*/*/*.csv\")\n",
    "nor_dir_lst = glob.glob(\"./data/normal/*/*/*.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a03e5a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "ab_folder = glob.glob(\"./data/abnormal/*/*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "169971ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# glob.glob(\"./data/abnormal/*/G_AIR_241115_F02log005_H100/*.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "32200ba4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>TimeUS</th>\n",
       "      <th>TPD</th>\n",
       "      <th>PD</th>\n",
       "      <th>DVD</th>\n",
       "      <th>TVD</th>\n",
       "      <th>VD</th>\n",
       "      <th>DAD</th>\n",
       "      <th>TAD</th>\n",
       "      <th>AD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-11-15 14:29:38</td>\n",
       "      <td>105291388</td>\n",
       "      <td>-0.456661</td>\n",
       "      <td>-0.456661</td>\n",
       "      <td>0.010541</td>\n",
       "      <td>0.010541</td>\n",
       "      <td>0.010541</td>\n",
       "      <td>0.002933</td>\n",
       "      <td>0.002933</td>\n",
       "      <td>0.002933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-11-15 14:29:38</td>\n",
       "      <td>105391414</td>\n",
       "      <td>-0.457436</td>\n",
       "      <td>-0.457436</td>\n",
       "      <td>0.010425</td>\n",
       "      <td>0.010425</td>\n",
       "      <td>0.010425</td>\n",
       "      <td>0.004455</td>\n",
       "      <td>0.004455</td>\n",
       "      <td>0.004455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-11-15 14:29:38</td>\n",
       "      <td>105491441</td>\n",
       "      <td>-0.456347</td>\n",
       "      <td>-0.456347</td>\n",
       "      <td>0.011239</td>\n",
       "      <td>0.011239</td>\n",
       "      <td>0.011239</td>\n",
       "      <td>0.004006</td>\n",
       "      <td>0.004006</td>\n",
       "      <td>0.004006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024-11-15 14:29:39</td>\n",
       "      <td>105591410</td>\n",
       "      <td>-0.454285</td>\n",
       "      <td>-0.454285</td>\n",
       "      <td>0.012009</td>\n",
       "      <td>0.012009</td>\n",
       "      <td>0.012009</td>\n",
       "      <td>0.002805</td>\n",
       "      <td>0.002805</td>\n",
       "      <td>0.002805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024-11-15 14:29:39</td>\n",
       "      <td>105691451</td>\n",
       "      <td>-0.454384</td>\n",
       "      <td>-0.454384</td>\n",
       "      <td>0.011582</td>\n",
       "      <td>0.011582</td>\n",
       "      <td>0.011582</td>\n",
       "      <td>-0.003383</td>\n",
       "      <td>-0.003383</td>\n",
       "      <td>-0.003383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12648</th>\n",
       "      <td>2024-11-15 14:50:43</td>\n",
       "      <td>1370091412</td>\n",
       "      <td>0.305536</td>\n",
       "      <td>0.305536</td>\n",
       "      <td>0.002092</td>\n",
       "      <td>0.002092</td>\n",
       "      <td>0.002092</td>\n",
       "      <td>0.001940</td>\n",
       "      <td>0.001940</td>\n",
       "      <td>0.001940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12649</th>\n",
       "      <td>2024-11-15 14:50:43</td>\n",
       "      <td>1370191512</td>\n",
       "      <td>0.306629</td>\n",
       "      <td>0.306629</td>\n",
       "      <td>0.002252</td>\n",
       "      <td>0.002252</td>\n",
       "      <td>0.002252</td>\n",
       "      <td>-0.001161</td>\n",
       "      <td>-0.001161</td>\n",
       "      <td>-0.001161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12650</th>\n",
       "      <td>2024-11-15 14:50:43</td>\n",
       "      <td>1370291463</td>\n",
       "      <td>0.307871</td>\n",
       "      <td>0.307871</td>\n",
       "      <td>0.001355</td>\n",
       "      <td>0.001355</td>\n",
       "      <td>0.001355</td>\n",
       "      <td>0.000737</td>\n",
       "      <td>0.000737</td>\n",
       "      <td>0.000737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12651</th>\n",
       "      <td>2024-11-15 14:50:43</td>\n",
       "      <td>1370391479</td>\n",
       "      <td>0.309397</td>\n",
       "      <td>0.309397</td>\n",
       "      <td>0.001391</td>\n",
       "      <td>0.001391</td>\n",
       "      <td>0.001391</td>\n",
       "      <td>-0.005319</td>\n",
       "      <td>-0.005319</td>\n",
       "      <td>-0.005319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12652</th>\n",
       "      <td>2024-11-15 14:50:43</td>\n",
       "      <td>1370491501</td>\n",
       "      <td>0.310286</td>\n",
       "      <td>0.310286</td>\n",
       "      <td>0.001260</td>\n",
       "      <td>0.001260</td>\n",
       "      <td>0.001260</td>\n",
       "      <td>0.007850</td>\n",
       "      <td>0.007850</td>\n",
       "      <td>0.007850</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12653 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 timestamp      TimeUS       TPD        PD       DVD  \\\n",
       "0      2024-11-15 14:29:38   105291388 -0.456661 -0.456661  0.010541   \n",
       "1      2024-11-15 14:29:38   105391414 -0.457436 -0.457436  0.010425   \n",
       "2      2024-11-15 14:29:38   105491441 -0.456347 -0.456347  0.011239   \n",
       "3      2024-11-15 14:29:39   105591410 -0.454285 -0.454285  0.012009   \n",
       "4      2024-11-15 14:29:39   105691451 -0.454384 -0.454384  0.011582   \n",
       "...                    ...         ...       ...       ...       ...   \n",
       "12648  2024-11-15 14:50:43  1370091412  0.305536  0.305536  0.002092   \n",
       "12649  2024-11-15 14:50:43  1370191512  0.306629  0.306629  0.002252   \n",
       "12650  2024-11-15 14:50:43  1370291463  0.307871  0.307871  0.001355   \n",
       "12651  2024-11-15 14:50:43  1370391479  0.309397  0.309397  0.001391   \n",
       "12652  2024-11-15 14:50:43  1370491501  0.310286  0.310286  0.001260   \n",
       "\n",
       "            TVD        VD       DAD       TAD        AD  \n",
       "0      0.010541  0.010541  0.002933  0.002933  0.002933  \n",
       "1      0.010425  0.010425  0.004455  0.004455  0.004455  \n",
       "2      0.011239  0.011239  0.004006  0.004006  0.004006  \n",
       "3      0.012009  0.012009  0.002805  0.002805  0.002805  \n",
       "4      0.011582  0.011582 -0.003383 -0.003383 -0.003383  \n",
       "...         ...       ...       ...       ...       ...  \n",
       "12648  0.002092  0.002092  0.001940  0.001940  0.001940  \n",
       "12649  0.002252  0.002252 -0.001161 -0.001161 -0.001161  \n",
       "12650  0.001355  0.001355  0.000737  0.000737  0.000737  \n",
       "12651  0.001391  0.001391 -0.005319 -0.005319 -0.005319  \n",
       "12652  0.001260  0.001260  0.007850  0.007850  0.007850  \n",
       "\n",
       "[12653 rows x 10 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(ab_dir_lst[2])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2ca8112b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_dir = glob.glob(\"./data/abnormal/output_csv/*\")[0]\n",
    "\n",
    "sample_csv = sorted(glob.glob(f\"{sample_dir}/*.csv\"))\n",
    "len(sample_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f5e3e43c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12653"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pd.read_csv(sample_csv[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0c011f08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Parse error in file: ./data/abnormal/output_csv/G_AIR_241115_F02log005_H100/G_AIR_241115_F02log005_H100_FMT.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "csv_len_dict = {}\n",
    "bad_files = []\n",
    "\n",
    "for csv in sample_csv:\n",
    "    try:\n",
    "        file = pd.read_csv(csv)\n",
    "        l = len(file)\n",
    "        csv_len_dict[l] = csv_len_dict.get(l, 0) + 1\n",
    "    except pd.errors.ParserError as e:\n",
    "        print(f\"❌ Parse error in file: {csv}\")\n",
    "        bad_files.append(csv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a417de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'G_AIR_241115_F02log005_H100_AHR2.csv'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.basename(sample_csv[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e3ba30ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# {행 개수: [메시지 이름, 메시지 이름, ...]} 형태로 저장\n",
    "rowcount_to_msgtype = {}\n",
    "\n",
    "for csv in sample_csv:\n",
    "    try:\n",
    "        # 파일명에서 메시지 이름 추출\n",
    "        msg_name = os.path.basename(csv).split('_')[-1].replace('.csv', '')\n",
    "        \n",
    "        # CSV 로드\n",
    "        df = pd.read_csv(csv, on_bad_lines='skip')  # 오류 있는 줄은 생략\n",
    "        row_count = len(df)\n",
    "        \n",
    "        # 딕셔너리에 추가\n",
    "        if row_count not in rowcount_to_msgtype:\n",
    "            rowcount_to_msgtype[row_count] = []\n",
    "        rowcount_to_msgtype[row_count].append(msg_name)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"{csv} 처리 중 오류: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8f41e870",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12653 rows: ['AHR2', 'ATT', 'CTRL', 'MOTB', 'POS', 'PSCD', 'PSCE', 'PSCN', 'RATE', 'RCI2', 'RCIN', 'RCO2', 'RCOU', 'VIBE', 'XKF1', 'XKF2', 'XKF3', 'XKF4', 'XKF5', 'XKFS', 'XKQ']\n",
      "1 rows: ['ARM', 'CAND', 'MODE', 'VER']\n",
      "12652 rows: ['BARO', 'BAT', 'CTUN', 'DCM', 'EAHR', 'FTN1', 'MAG', 'MCU', 'POWR']\n",
      "1266 rows: ['CANS', 'DU32', 'TERR']\n",
      "1263 rows: ['DSF']\n",
      "8 rows: ['EV']\n",
      "3 rows: ['FILE', 'FMT']\n",
      "176 rows: ['FMTU']\n",
      "37956 rows: ['FTN2']\n",
      "6326 rows: ['GPA', 'GPS']\n",
      "1265 rows: ['HEAT']\n",
      "31631 rows: ['IMU']\n",
      "1205 rows: ['IOMC']\n",
      "2524 rows: ['MAV']\n",
      "124 rows: ['MAVC']\n",
      "14 rows: ['MSG']\n",
      "15 rows: ['MULT']\n",
      "2 rows: ['ORGN']\n",
      "1190 rows: ['PARM']\n",
      "127 rows: ['PM']\n",
      "7 rows: ['SRTL']\n",
      "12541 rows: ['STAK']\n",
      "126 rows: ['TSYN']\n",
      "36 rows: ['UNIT']\n",
      "248 rows: ['XKT']\n",
      "2531 rows: ['XKV1', 'XKV2']\n",
      "12651 rows: ['XKY0', 'XKY1']\n"
     ]
    }
   ],
   "source": [
    "for count, msgs in rowcount_to_msgtype.items():\n",
    "    print(f\"{count} rows: {msgs}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c2c10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_len_dict = {}\n",
    "\n",
    "for dir in ab_folder:\n",
    "    csv_lst = glob.glob(f\"{dir}/*.csv\")\n",
    "    csv_len_dict = {csv: {} for csv in csv_lst}\n",
    "    for csv in csv_lst:\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7a7c7452",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 'mergeable_message_summary.csv' 생성 완료!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "summary_stats = []\n",
    "\n",
    "root_dir = \"./data\"\n",
    "\n",
    "for category in ['abnormal', 'normal']:\n",
    "    base_path = os.path.join(root_dir, category, \"output_csv\")\n",
    "\n",
    "    for log_folder in os.listdir(base_path):\n",
    "        folder_path = os.path.join(base_path, log_folder)\n",
    "        if not os.path.isdir(folder_path):\n",
    "            continue\n",
    "\n",
    "        row_count_to_msgs = defaultdict(list)\n",
    "\n",
    "        for file in os.listdir(folder_path):\n",
    "            if file.endswith(\".csv\"):\n",
    "                msg_name = file.split('_')[-1].replace('.csv', '')\n",
    "                file_path = os.path.join(folder_path, file)\n",
    "\n",
    "                try:\n",
    "                    df = pd.read_csv(file_path, on_bad_lines='skip')\n",
    "                    row_count = len(df)\n",
    "                    row_count_to_msgs[row_count].append(msg_name)\n",
    "                except Exception as e:\n",
    "                    print(f\"⚠️ {file_path} 읽기 실패: {e}\")\n",
    "                    continue\n",
    "\n",
    "        if not row_count_to_msgs:\n",
    "            continue\n",
    "\n",
    "        # 가장 많은 메시지 수를 가진 row count 찾기\n",
    "        most_common_row_count = max(row_count_to_msgs.items(), key=lambda x: len(x[1]))[0]\n",
    "        max_msgs = sorted(row_count_to_msgs[most_common_row_count])\n",
    "\n",
    "        summary_stats.append({\n",
    "            \"category\": category,\n",
    "            \"log_folder\": log_folder,\n",
    "            \"most_common_row_count\": most_common_row_count,\n",
    "            \"num_messages\": len(max_msgs),\n",
    "            \"message_list\": max_msgs\n",
    "        })\n",
    "\n",
    "# 결과 저장\n",
    "df_summary = pd.DataFrame(summary_stats)\n",
    "df_summary = df_summary.sort_values(by=[\"category\", \"log_folder\"])\n",
    "df_summary.to_csv(\"mergeable_message_summary.csv\", index=False)\n",
    "\n",
    "print(\"✅ 'mergeable_message_summary.csv' 생성 완료!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5f273af5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 모든 로그에서 공통으로 병합 가능한 메시지 목록:\n",
      "['AHR2', 'MOTB', 'POS', 'PSCD', 'RCO2', 'RCOU', 'VIBE']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "from functools import reduce\n",
    "\n",
    "# CSV 불러오기\n",
    "df = pd.read_csv(\"mergeable_message_summary.csv\")\n",
    "\n",
    "# 문자열 형태의 리스트를 실제 리스트로 변환\n",
    "df['message_list'] = df['message_list'].apply(ast.literal_eval)\n",
    "\n",
    "# 모든 message_list들의 교집합 구하기\n",
    "common_messages = list(reduce(lambda x, y: set(x) & set(y), df['message_list']))\n",
    "\n",
    "# 결과 출력\n",
    "print(\"✅ 모든 로그에서 공통으로 병합 가능한 메시지 목록:\")\n",
    "print(sorted(common_messages))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66391a8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d2c197",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from pymavlink import mavutil\n",
    "\n",
    "\n",
    "def kmj_paser():\n",
    "    start_time = time.time()\n",
    "    # bin 파일 읽기\n",
    "    mlog = mavutil.mavlink_connection(\"/home/gaion/Desktop/기타/parser/normal/G_AIR_241112_N00log001_H000.bin\")\n",
    "    no_need_type = [\"FMT\", \"FILE\", \"CAND\", \"FMTU\", \"UNIT\" ,\"MSG\"]\n",
    "    msg_set = dict()\n",
    "    while True: # bin 파일 로그 한 줄씩 루프\n",
    "        msg = mlog.recv_match(blocking=True)\n",
    "        if msg is None: # 타입이 없는 경우 -> 로그 마지막 의미\n",
    "            break\n",
    "\n",
    "        msg_type = msg.get_type()\n",
    "        msg_data = msg.to_dict()\n",
    "\n",
    "        if msg_type in no_need_type: # 타입이 no_need_type에 있는 경우에는 다음 루프로 넘어감 아래 생략\n",
    "            continue\n",
    "\n",
    "        if msg_type not in msg_set: # 타입이 msg_set에 없고, no_need_type에 없는 경우\n",
    "            msg_set[msg_type] = []  # msg 딕셔너리에 type을 키, 해당 type의 메시지를 값으로 설정\n",
    "            msg_set[msg_type].append(msg_data)\n",
    "\n",
    "    # 메시지 타입과  타입별 데이터 갯수 확인하기\n",
    "    count_list = dict()\n",
    "    for type in msg_set:\n",
    "        count = len(msg_set[type])\n",
    "        count_list[type] = count\n",
    "\n",
    "    # 타입별 데이터 갯수 기준으로 타입명 정렬 return list\n",
    "    sorted_type = sorted(count_list, key=lambda k: count_list[k], reverse=True)\n",
    "    print(sorted_type)\n",
    "\n",
    "    time2 = time.time()\n",
    "    print(1111111, time2 - start_time)\n",
    "    df_merge = pd.DataFrame()  #merge 할 기본 데이터프레임(가장 행 갯수가 많은 타입으로) 생성\n",
    "    for type in sorted_type: # 정렬된 타입 순 리스트를 순회하며 값을 데이터 프레임으로 변환\n",
    "        df = pd.DataFrame(msg_set[type]) # msg_set[type] 의 value에 해당하는 데이터을 데이터프레임으로 변환 하고 df로 저장\n",
    "\n",
    "        # 데이터 프레임 전처리(\n",
    "        prefix = df['mavpackettype'][0]\n",
    "        df = df.rename(columns={col: f'{prefix}_{col}' for col in list(df.columns)[2:]})  # 1. mavpackkettpye,TimeUS 컬럼을 제외하고 나머지 칼럼에 해당 macpackettype 명으로 prefix로 붙여줌\n",
    "        df = df.drop(columns=df.columns[0]) #2. mavpackettype 열 삭제\n",
    "        # msg_set[type] = df  # 전처리 완료한 데이터프레임을 msg_set[type]에 다시 저장\n",
    "        if type != sorted_type[0]:\n",
    "            df_merge = pd.merge_asof(df_merge, df, on='TimeUS', direction='forward')  # 첫번째 타입이 아닐 경우 df_merge와 df를 병합\n",
    "        else:\n",
    "            df_merge = df  #첫번째 타입은 df_merge에 저장\n",
    "\n",
    "    return df_merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f56ecac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./origin_bin/normal/G_AIR_241112_N00log001_H000.bin',\n",
       " './origin_bin/normal/G_AIR_241112_N00log003_H000.BIN',\n",
       " './origin_bin/normal/G_AIR_241112_N00log002_H000.bin',\n",
       " './origin_bin/normal/G_AIR_241112_N00log005_H000.bin',\n",
       " './origin_bin/normal/G_AIR_241112_N00log004_H000.BIN']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "normal_dir = glob.glob(\"./origin_bin/normal/*\")\n",
    "normal_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d115ff9b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
